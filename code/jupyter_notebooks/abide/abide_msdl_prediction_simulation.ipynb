{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import datasets, plotting\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from collections import deque, defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/ajjain/nilearn_data/msdl_atlas\n"
     ]
    }
   ],
   "source": [
    "home_base_dir = '/Users/aj/dmello_lab/fmri_connectivity_trees' # directory where repository lives at home computer\n",
    "lab_base_dir = '/Users/ajjain/Downloads/Code/fmri_connectivity_trees' # directory where repository lives at lab computer\n",
    "\n",
    "# set base directory depending on where the code is being run\n",
    "base_dir = home_base_dir if os.path.exists(home_base_dir) else lab_base_dir\n",
    "\n",
    "# get msdl and whole brain atlases and coords\n",
    "msdl_data = datasets.fetch_atlas_msdl()\n",
    "msdl_coords = msdl_data.region_coords\n",
    "\n",
    "# path for shapes and pooled timeseries\n",
    "cort_shape_path = f'{base_dir}/code/functional_connectivity/abide/output/roi_time_series/884_MSDL/shape'\n",
    "cort_pooled_path = f'{base_dir}/code/functional_connectivity/abide/output/roi_time_series/884_MSDL/pooled'\n",
    "\n",
    "# load abide ids\n",
    "with open(f'/Users/ajjain/Downloads/Code/fmri_connectivity_trees/datasets/abide/phenotypic/abide_ids.txt', 'r') as f:\n",
    "    abide_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# load mapped phenotypes\n",
    "with open(f'{base_dir}/datasets/abide/phenotypic/phenotype.txt', 'r') as f:\n",
    "    phenotype = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# load saved covariance for entire dataset\n",
    "asd_cov = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/covariance/msdl/asd_cov_msdl.csv').to_numpy()\n",
    "tdc_cov = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/covariance/msdl/tdc_cov_msdl.csv').to_numpy()\n",
    "\n",
    "# load saved mutual information for entire dataset\n",
    "asd_mi = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/mutual_info/msdl/1000_bins/asd_mutual_info.csv').to_numpy()\n",
    "tdc_mi = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/mutual_info/msdl/1000_bins/tdc_mutual_info.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate timeseries for asd and tdc\n",
    "def get_groups(abide_ids, phenotype):\n",
    "    asd = []\n",
    "    tdc = []\n",
    "    for i in range(len(abide_ids)):\n",
    "        if phenotype[i] == 1:\n",
    "            asd.append(abide_ids[i])\n",
    "        else: tdc.append(abide_ids[i])\n",
    "    asd = np.array(asd)\n",
    "    tdc = np.array(tdc)\n",
    "\n",
    "    return {'asd': asd, 'tdc': tdc}\n",
    "\n",
    "# concatenate timeseries to get a single timeseries for each subject\n",
    "def get_timeseries(ids, cort_shape_path=cort_shape_path, cort_pooled_path=cort_pooled_path):\n",
    "    timeseries = []\n",
    "    for id in ids:\n",
    "\n",
    "        # load the shape and pooled timeseries\n",
    "        shape = np.loadtxt(f'{cort_shape_path}/{id}.csv', delimiter=',').astype(int)\n",
    "        pooled = np.loadtxt(f'{cort_pooled_path}/{id}.csv', delimiter=',').reshape(shape)\n",
    "\n",
    "        # concatenate the timeseries\n",
    "        timeseries.append(pooled)\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "# concatenate into one time series\n",
    "def concat(timeseries):\n",
    "    concat_timeseries = timeseries[0]\n",
    "    for series in timeseries[1:]:\n",
    "        concat_timeseries = np.concatenate((concat_timeseries, series), axis=0)\n",
    "    return concat_timeseries\n",
    "\n",
    "# get separate ids for each group\n",
    "abide_groups = get_groups(abide_ids, phenotype)\n",
    "\n",
    "# get timeseries for each group\n",
    "asd_timeseries = get_timeseries(abide_groups['asd']) # asd timeseries\n",
    "tdc_timeseries = get_timeseries(abide_groups['tdc']) # tdc timeseries\n",
    "\n",
    "# all timeseries concatenated\n",
    "asd_concat = concat(asd_timeseries) # asd concatenated timeseries\n",
    "tdc_concat = concat(tdc_timeseries) # tdc concatenated timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get covariance matrix for a given timeseries\n",
    "def get_covariance(concat_timeseries):\n",
    "    \"\"\"\n",
    "    Get the covariance matrix of the concatenated timeseries.\n",
    "    \"\"\"\n",
    "    # get the covariance matrix\n",
    "    cov = np.cov(concat_timeseries.T)\n",
    "\n",
    "    for i in range(len(cov)):\n",
    "        for j in range(len(cov)):\n",
    "            if i == j:\n",
    "                cov[i][j] = 0\n",
    "\n",
    "    return cov\n",
    "\n",
    "# helper function for mutual info between two continuous random variables\n",
    "def mutual_information_continuous(x, y, bins=100):\n",
    "    \"\"\"\n",
    "    Estimate mutual information between two continuous variables by discretizing them.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): First continuous variable.\n",
    "    y (array-like): Second continuous variable.\n",
    "    bins (int): Number of bins to use for discretization.\n",
    "\n",
    "    Returns:\n",
    "    float: Estimated mutual information.\n",
    "    \"\"\"\n",
    "    # Convert to pandas Series\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Discretize the continuous values\n",
    "    x_binned = pd.cut(x, bins=bins, labels=False)\n",
    "    y_binned = pd.cut(y, bins=bins, labels=False)\n",
    "\n",
    "    # Compute contingency table\n",
    "    contingency_table = pd.crosstab(x_binned, y_binned)\n",
    "\n",
    "    # Compute mutual information\n",
    "    mi = mutual_info_score(None, None, contingency=contingency_table.values)\n",
    "\n",
    "    return mi\n",
    "\n",
    "# get mutual information between all regions for a given timeseries\n",
    "def get_mutual_info(timeseries, bins=1000):\n",
    "\n",
    "    mutual_info_matrix = np.zeros((timeseries.shape[1], timeseries.shape[1]))\n",
    "\n",
    "    # get mutual information\n",
    "    for region in range(timeseries.shape[1]):\n",
    "        for region2 in range(timeseries.shape[1]):\n",
    "            if region != region2:\n",
    "                mutual_info = mutual_information_continuous(timeseries[:, region], timeseries[:, region2], bins=bins)\n",
    "                mutual_info_matrix[region, region2] = mutual_info\n",
    "    \n",
    "    return mutual_info_matrix\n",
    "\n",
    "# Construct the Chow-Liu tree from continuous data using mutual information.\n",
    "def get_chow_liu_tree(timeseries, bins=1000):\n",
    "    \n",
    "    # Compute the mutual information matrix\n",
    "    mi_matrix = get_mutual_info(timeseries, bins=bins)\n",
    "\n",
    "    # Set the diagonal to zero (self-information)\n",
    "    np.fill_diagonal(mi_matrix, 0)\n",
    "\n",
    "    # We use the negative MI because scipy's minimum_spanning_tree computes *minimum* tree.\n",
    "    mst = minimum_spanning_tree(-mi_matrix).toarray()\n",
    "\n",
    "    # Make MST undirected and remove negative sign\n",
    "    mst = -mst + (-mst).T\n",
    "\n",
    "    # Create NetworkX graph for visualization or further use\n",
    "    G = nx.Graph()\n",
    "    n = mi_matrix.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if mst[i, j] != 0:\n",
    "                G.add_edge(i, j, weight=mi_matrix[i, j])\n",
    "    \n",
    "    return G, mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate timeseries for prediction dataset\n",
    "def truncate_timeseries(tdc_timeseries, asd_timeseries, fraction=0.1):\n",
    "\n",
    "    n_tdc = int(len(tdc_timeseries) * fraction)\n",
    "    n_asd = int(len(asd_timeseries) * fraction)\n",
    "\n",
    "    # select n_tdc random timeseries\n",
    "    tdc_indices = np.random.choice(len(tdc_timeseries), n_tdc, replace=False)\n",
    "    tdc_timeseries_trunc = [tdc_timeseries[i] for i in tdc_indices]\n",
    "\n",
    "    # select n_asd random timeseries\n",
    "    asd_indices = np.random.choice(len(asd_timeseries), n_asd, replace=False)\n",
    "    asd_timeseries_trunc = [asd_timeseries[i] for i in asd_indices]\n",
    "\n",
    "    return tdc_timeseries_trunc, asd_timeseries_trunc\n",
    "\n",
    "# get cross validation split\n",
    "def cross_validation_split(tdc_timeseries_trunc, asd_timeseries_trunc, hold_out = 0.2):\n",
    "\n",
    "    # initialize training and test sets\n",
    "    tdc_training_set = []\n",
    "    tdc_test_set = []\n",
    "    asd_training_set = []\n",
    "    asd_test_set = []\n",
    "\n",
    "    # get number of training and test samples\n",
    "    tdc_train_num = int(len(tdc_timeseries_trunc) * (1 - hold_out))\n",
    "    asd_train_num = int(len(asd_timeseries_trunc) * (1 - hold_out))\n",
    "\n",
    "    # randomly shuffle timeseries\n",
    "    tdc_indices = np.random.choice(len(tdc_timeseries_trunc), len(tdc_timeseries_trunc), replace=False)\n",
    "    asd_indices = np.random.choice(len(asd_timeseries_trunc), len(asd_timeseries_trunc), replace=False)\n",
    "\n",
    "    # split into training and test sets\n",
    "    tdc_train_indices = tdc_indices[:tdc_train_num]\n",
    "    tdc_test_indices = tdc_indices[tdc_train_num:]\n",
    "    asd_train_indices = asd_indices[:asd_train_num]\n",
    "    asd_test_indices = asd_indices[asd_train_num:]\n",
    "\n",
    "    # get training and test sets\n",
    "    for i in tdc_train_indices:\n",
    "        tdc_training_set.append(tdc_timeseries_trunc[i])\n",
    "    for i in tdc_test_indices:\n",
    "        tdc_test_set.append(tdc_timeseries_trunc[i])\n",
    "    for i in asd_train_indices:\n",
    "        asd_training_set.append(asd_timeseries_trunc[i])\n",
    "    for i in asd_test_indices:\n",
    "        asd_test_set.append(asd_timeseries_trunc[i])\n",
    "    \n",
    "    # return concatenated timeseries for each\n",
    "    tdc_train_concat = concat(tdc_training_set)\n",
    "    tdc_test_concat = concat(tdc_test_set)\n",
    "    asd_train_concat = concat(asd_training_set)\n",
    "    asd_test_concat = concat(asd_test_set)\n",
    "\n",
    "    data = {\"tdc_train\": tdc_train_concat, \"tdc_test\": tdc_test_concat, \"asd_train\": asd_train_concat, \"asd_test\": asd_test_concat}\n",
    "    return data\n",
    "\n",
    "# separate concat timeseries into bins to create train and test set\n",
    "def bin_data(data, bin_num = 100, hold_out = 0.2):\n",
    "\n",
    "    train_bin_num = int(bin_num * (1 - hold_out))\n",
    "    test_bin_num = int(bin_num * hold_out)\n",
    "\n",
    "    # bin data\n",
    "    tdc_train_bins = np.array_split(data['tdc_train'], train_bin_num)\n",
    "    tdc_test_bins = np.array_split(data['tdc_test'], test_bin_num)\n",
    "    asd_train_bins = np.array_split(data['asd_train'], train_bin_num)\n",
    "    asd_test_bins = np.array_split(data['asd_test'], test_bin_num)\n",
    "\n",
    "    # return data\n",
    "    data = {\"tdc_train\": tdc_train_bins, \"tdc_test\": tdc_test_bins, \"asd_train\": asd_train_bins, \"asd_test\": asd_test_bins}\n",
    "    return data\n",
    "\n",
    "# consolidate\n",
    "def data_split(tdc_timeseries, asd_timeseries, fraction=0.1, hold_out=0.2, bin_num=100):\n",
    "\n",
    "    # truncate timeseries\n",
    "    tdc_timeseries_trunc, asd_timeseries_trunc = truncate_timeseries(tdc_timeseries, asd_timeseries, fraction=fraction)\n",
    "\n",
    "    # get cross validation split\n",
    "    data = cross_validation_split(tdc_timeseries_trunc, asd_timeseries_trunc, hold_out=hold_out)\n",
    "\n",
    "    # bin data\n",
    "    data = bin_data(data, bin_num=bin_num, hold_out=hold_out)\n",
    "\n",
    "    return data\n",
    "\n",
    "# shuffle asd and tdc, assign labels\n",
    "def shuffle_labels(data):\n",
    "\n",
    "    # shuffle the labels\n",
    "    tdc_train_labels = np.zeros(len(data['tdc_train']))\n",
    "    tdc_test_labels = np.zeros(len(data['tdc_test']))\n",
    "    asd_train_labels = np.ones(len(data['asd_train']))\n",
    "    asd_test_labels = np.ones(len(data['asd_test']))\n",
    "\n",
    "    # concatenate the data and labels\n",
    "    train_data = data['tdc_train'] + data['asd_train']\n",
    "    test_data = data['tdc_test'] + data['asd_test']\n",
    "    train_labels = np.concatenate((tdc_train_labels, asd_train_labels), axis=0)\n",
    "    test_labels = np.concatenate((tdc_test_labels, asd_test_labels), axis=0)\n",
    "\n",
    "    # shuffle the data and labels\n",
    "    indices = np.arange(len(train_data))\n",
    "    np.random.shuffle(indices)\n",
    "    train_data = [train_data[i] for i in indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    indices = np.arange(len(test_data))\n",
    "    np.random.shuffle(indices)\n",
    "    test_data = [test_data[i] for i in indices]\n",
    "    test_labels = test_labels[indices]\n",
    "    \n",
    "    data = {\"train_data\": train_data, \"test_data\": test_data, \"train_labels\": train_labels, \"test_labels\": test_labels}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple logistic regression for prediction\n",
    "def binary_prediction_model(data, model=GaussianNB(), metric=\"covariance\", mi_bin_num=100):\n",
    "\n",
    "    # get the data\n",
    "    train_data = data['train_data']\n",
    "    test_data = data['test_data']\n",
    "    train_labels = data['train_labels']\n",
    "    test_labels = data['test_labels']\n",
    "\n",
    "    # calculate metric (feature extraction)\n",
    "    if metric == \"covariance\":\n",
    "        train_data = np.array([get_covariance(series).flatten() for series in train_data])\n",
    "        test_data = np.array([get_covariance(series).flatten() for series in test_data])\n",
    "    elif metric == \"mutual_info\":\n",
    "        train_data = np.array([get_mutual_info(series, bins=mi_bin_num).flatten() for series in train_data])\n",
    "        test_data = np.array([get_mutual_info(series, bins=mi_bin_num).flatten() for series in test_data])\n",
    "    elif metric == \"chow_liu\":\n",
    "        _, train_data = np.array([get_chow_liu_tree(series, bins=mi_bin_num).flatten() for series in train_data])\n",
    "        _, test_data = np.array([get_chow_liu_tree(series, bins=mi_bin_num).flatten() for series in test_data])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Choose 'covariance', 'mutual_info', or 'chow-liu'.\")\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    # make predictions\n",
    "    train_predictions = model.predict(train_data)\n",
    "    test_predictions = model.predict(test_data)\n",
    "\n",
    "    # get accuracy\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "def run_model(tdc_timeseries, asd_timeseries, model=GaussianNB(), metric=\"covariance\", fraction=0.1, hold_out=0.2, bin_num=1000, mi_bin_num=100):\n",
    "\n",
    "    # get the data\n",
    "    data = data_split(tdc_timeseries, asd_timeseries, fraction=fraction, hold_out=hold_out, bin_num=bin_num)\n",
    "\n",
    "    # shuffle the labels\n",
    "    labeled_data = shuffle_labels(data)\n",
    "\n",
    "    # run logistic regression\n",
    "    train_accuracy, test_accuracy = binary_prediction_model(labeled_data, model=model, metric=metric, mi_bin_num=mi_bin_num)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_cov, test_accuracy_cov = run_model(tdc_timeseries, asd_timeseries, bin_num=1000, metric=\"covariance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5225"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_accuracy_mi, test_accuracy_mi = \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtdc_timeseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masd_timeseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmutual_info\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhold_out\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_num\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmi_bin_num\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_model\u001b[39m\u001b[34m(tdc_timeseries, asd_timeseries, model, metric, fraction, hold_out, bin_num, mi_bin_num)\u001b[39m\n\u001b[32m     42\u001b[39m labeled_data = shuffle_labels(data)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# run logistic regression\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m train_accuracy, test_accuracy = \u001b[43mbinary_prediction_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmi_bin_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmi_bin_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_accuracy, test_accuracy\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mbinary_prediction_model\u001b[39m\u001b[34m(data, model, metric, mi_bin_num)\u001b[39m\n\u001b[32m     13\u001b[39m     test_data = np.array([get_covariance(series).flatten() \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33mmutual_info\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     train_data = np.array(\u001b[43m[\u001b[49m\u001b[43mget_mutual_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmi_bin_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     16\u001b[39m     test_data = np.array([get_mutual_info(series, bins=mi_bin_num).flatten() \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33mchow_liu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     13\u001b[39m     test_data = np.array([get_covariance(series).flatten() \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33mmutual_info\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     train_data = np.array([\u001b[43mget_mutual_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmi_bin_num\u001b[49m\u001b[43m)\u001b[49m.flatten() \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[32m     16\u001b[39m     test_data = np.array([get_mutual_info(series, bins=mi_bin_num).flatten() \u001b[38;5;28;01mfor\u001b[39;00m series \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric == \u001b[33m\"\u001b[39m\u001b[33mchow_liu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mget_mutual_info\u001b[39m\u001b[34m(timeseries, bins)\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m region2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timeseries.shape[\u001b[32m1\u001b[39m]):\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m region != region2:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m             mutual_info = \u001b[43mmutual_information_continuous\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m             mutual_info_matrix[region, region2] = mutual_info\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mutual_info_matrix\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mmutual_information_continuous\u001b[39m\u001b[34m(x, y, bins)\u001b[39m\n\u001b[32m     35\u001b[39m y_binned = pd.cut(y, bins=bins, labels=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Compute contingency table\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m contingency_table = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcrosstab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_binned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_binned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Compute mutual information\u001b[39;00m\n\u001b[32m     41\u001b[39m mi = mutual_info_score(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, contingency=contingency_table.values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:728\u001b[39m, in \u001b[36mcrosstab\u001b[39m\u001b[34m(index, columns, values, rownames, colnames, aggfunc, margins, margins_name, dropna, normalize)\u001b[39m\n\u001b[32m    724\u001b[39m     kwargs = {\u001b[33m\"\u001b[39m\u001b[33maggfunc\u001b[39m\u001b[33m\"\u001b[39m: aggfunc}\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# error: Argument 7 to \"pivot_table\" of \"DataFrame\" has incompatible type\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# \"**Dict[str, object]\"; expected \"Union[...]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m table = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__dummy__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_rownames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43munique_colnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    737\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# Post-process\u001b[39;00m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/frame.py:9509\u001b[39m, in \u001b[36mDataFrame.pivot_table\u001b[39m\u001b[34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[39m\n\u001b[32m   9492\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   9493\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mpivot_table\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   9494\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpivot_table\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m   9505\u001b[39m     sort: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   9506\u001b[39m ) -> DataFrame:\n\u001b[32m   9507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[32m-> \u001b[39m\u001b[32m9509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9510\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9514\u001b[39m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9520\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9521\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:102\u001b[39m, in \u001b[36mpivot_table\u001b[39m\u001b[34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[39m\n\u001b[32m     99\u001b[39m     table = concat(pieces, keys=keys, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m table.__finalize__(data, method=\u001b[33m\"\u001b[39m\u001b[33mpivot_table\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m table = \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m table.__finalize__(data, method=\u001b[33m\"\u001b[39m\u001b[33mpivot_table\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/reshape/pivot.py:183\u001b[39m, in \u001b[36m__internal_pivot_table\u001b[39m\u001b[34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    174\u001b[39m     ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouped._grouper.groupings\n\u001b[32m    175\u001b[39m ):\n\u001b[32m    176\u001b[39m     warnings.warn(\n\u001b[32m    177\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe default value of observed=False is deprecated and will change \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto observed=True in a future version of pandas. Specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m agged = \u001b[43mgrouped\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dropna \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(agged, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agged.columns):\n\u001b[32m    186\u001b[39m     agged = agged.dropna(how=\u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1466\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;66;03m# grouper specific aggregations\u001b[39;00m\n\u001b[32m   1464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grouper.nkeys > \u001b[32m1\u001b[39m:\n\u001b[32m   1465\u001b[39m     \u001b[38;5;66;03m# test_groupby_as_index_series_scalar gets here with 'not self.as_index'\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m kwargs:\n\u001b[32m   1468\u001b[39m     \u001b[38;5;66;03m# test_pass_args_kwargs gets here (with and without as_index)\u001b[39;00m\n\u001b[32m   1469\u001b[39m     \u001b[38;5;66;03m# can't return early\u001b[39;00m\n\u001b[32m   1470\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._aggregate_frame(func, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1517\u001b[39m, in \u001b[36mDataFrameGroupBy._python_agg_general\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m   1514\u001b[39m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[32m   1515\u001b[39m f = \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mngroups\u001b[49m == \u001b[32m0\u001b[39m:\n\u001b[32m   1518\u001b[39m     \u001b[38;5;66;03m# e.g. test_evaluate_with_empty_groups different path gets different\u001b[39;00m\n\u001b[32m   1519\u001b[39m     \u001b[38;5;66;03m#  result dtype in empty case.\u001b[39;00m\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._python_apply_general(f, \u001b[38;5;28mself\u001b[39m._selected_obj, is_agg=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1522\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._obj_with_exclusions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:856\u001b[39m, in \u001b[36mBaseGroupBy.ngroups\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    853\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mngroups\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mngroups\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:36\u001b[39m, in \u001b[36mpandas._libs.properties.CachedProperty.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/ops.py:774\u001b[39m, in \u001b[36mBaseGrouper.ngroups\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mngroups\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult_index\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:36\u001b[39m, in \u001b[36mpandas._libs.properties.CachedProperty.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/ops.py:787\u001b[39m, in \u001b[36mBaseGrouper.result_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.groupings) == \u001b[32m1\u001b[39m:\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.groupings[\u001b[32m0\u001b[39m]._result_index.rename(\u001b[38;5;28mself\u001b[39m.names[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m codes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreconstructed_codes\u001b[49m\n\u001b[32m    788\u001b[39m levels = [ping._result_index \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.groupings]\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MultiIndex(\n\u001b[32m    790\u001b[39m     levels=levels, codes=codes, verify_integrity=\u001b[38;5;28;01mFalse\u001b[39;00m, names=\u001b[38;5;28mself\u001b[39m.names\n\u001b[32m    791\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/ops.py:778\u001b[39m, in \u001b[36mBaseGrouper.reconstructed_codes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreconstructed_codes\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[npt.NDArray[np.intp]]:\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     codes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcodes\u001b[49m\n\u001b[32m    779\u001b[39m     ids, obs_ids, _ = \u001b[38;5;28mself\u001b[39m.group_info\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decons_obs_group_ids(ids, obs_ids, \u001b[38;5;28mself\u001b[39m.shape, codes, xnull=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/ops.py:690\u001b[39m, in \u001b[36mBaseGrouper.codes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[npt.NDArray[np.signedinteger]]:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroupings\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/ops.py:690\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    688\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[npt.NDArray[np.signedinteger]]:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.groupings]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:691\u001b[39m, in \u001b[36mGrouping.codes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> npt.NDArray[np.signedinteger]:\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codes_and_uniques\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:36\u001b[39m, in \u001b[36mpandas._libs.properties.CachedProperty.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:835\u001b[39m, in \u001b[36mGrouping._codes_and_uniques\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    830\u001b[39m     uniques = \u001b[38;5;28mself\u001b[39m._uniques\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    832\u001b[39m     \u001b[38;5;66;03m# GH35667, replace dropna=False with use_na_sentinel=False\u001b[39;00m\n\u001b[32m    833\u001b[39m     \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type \"Union[\u001b[39;00m\n\u001b[32m    834\u001b[39m     \u001b[38;5;66;03m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     codes, uniques = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgrouping_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dropna\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m codes, uniques\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/algorithms.py:795\u001b[39m, in \u001b[36mfactorize\u001b[39m\u001b[34m(values, sort, use_na_sentinel, size_hint)\u001b[39m\n\u001b[32m    792\u001b[39m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[32m    793\u001b[39m             values = np.where(null_mask, na_value, values)\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m     codes, uniques = \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m0\u001b[39m:\n\u001b[32m    802\u001b[39m     uniques, codes = safe_sort(\n\u001b[32m    803\u001b[39m         uniques,\n\u001b[32m    804\u001b[39m         codes,\n\u001b[32m   (...)\u001b[39m\u001b[32m    807\u001b[39m         verify=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    808\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dynamric/lib/python3.11/site-packages/pandas/core/algorithms.py:594\u001b[39m, in \u001b[36mfactorize_array\u001b[39m\u001b[34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[39m\n\u001b[32m    590\u001b[39m     na_value = iNaT\n\u001b[32m    592\u001b[39m hash_klass, values = _get_hashtable_algo(values)\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m table = \u001b[43mhash_klass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_hint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m uniques, codes = table.factorize(\n\u001b[32m    596\u001b[39m     values,\n\u001b[32m    597\u001b[39m     na_sentinel=-\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    600\u001b[39m     ignore_na=use_na_sentinel,\n\u001b[32m    601\u001b[39m )\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_accuracy_mi, test_accuracy_mi = run_model(tdc_timeseries, asd_timeseries, metric=\"mutual_info\", fraction=0.1, hold_out=0.2, bin_num=100, mi_bin_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.565"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy_mi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
