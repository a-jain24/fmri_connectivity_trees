{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import datasets, plotting\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from collections import deque, defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/aj/nilearn_data/msdl_atlas\n"
     ]
    }
   ],
   "source": [
    "home_base_dir = '/Users/aj/dmello_lab/fmri_connectivity_trees' # directory where repository lives at home computer\n",
    "lab_base_dir = '/Users/ajjain/Downloads/Code/fmri_connectivity_trees' # directory where repository lives at lab computer\n",
    "\n",
    "# set base directory depending on where the code is being run\n",
    "base_dir = home_base_dir if os.path.exists(home_base_dir) else lab_base_dir\n",
    "\n",
    "# get msdl and whole brain atlases and coords\n",
    "msdl_data = datasets.fetch_atlas_msdl()\n",
    "msdl_coords = msdl_data.region_coords\n",
    "\n",
    "# path for shapes and pooled timeseries\n",
    "cort_shape_path = f'{base_dir}/code/functional_connectivity/abide/output/roi_time_series/884_MSDL/shape'\n",
    "cort_pooled_path = f'{base_dir}/code/functional_connectivity/abide/output/roi_time_series/884_MSDL/pooled'\n",
    "\n",
    "# load abide ids\n",
    "with open(f'{base_dir}/datasets/abide/phenotypic/abide_ids.txt', 'r') as f:\n",
    "    abide_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# load mapped phenotypes\n",
    "with open(f'{base_dir}/datasets/abide/phenotypic/phenotype.txt', 'r') as f:\n",
    "    phenotype = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# load saved covariance for entire dataset\n",
    "asd_cov = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/covariance/msdl/asd_cov_msdl.csv').to_numpy()\n",
    "tdc_cov = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/covariance/msdl/tdc_cov_msdl.csv').to_numpy()\n",
    "\n",
    "# load saved mutual information for entire dataset\n",
    "asd_mi = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/mutual_info/msdl/1000_bins/asd_mutual_info.csv').to_numpy()\n",
    "tdc_mi = pd.read_csv(f'{base_dir}/code/functional_connectivity/abide/output/mutual_info/msdl/1000_bins/tdc_mutual_info.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate timeseries for asd and tdc\n",
    "def get_groups(abide_ids, phenotype):\n",
    "    asd = []\n",
    "    tdc = []\n",
    "    for i in range(len(abide_ids)):\n",
    "        if phenotype[i] == 1:\n",
    "            asd.append(abide_ids[i])\n",
    "        else: tdc.append(abide_ids[i])\n",
    "    asd = np.array(asd)\n",
    "    tdc = np.array(tdc)\n",
    "\n",
    "    return {'asd': asd, 'tdc': tdc}\n",
    "\n",
    "# concatenate timeseries to get a single timeseries for each subject\n",
    "def get_timeseries(ids, cort_shape_path=cort_shape_path, cort_pooled_path=cort_pooled_path):\n",
    "    timeseries = []\n",
    "    for id in ids:\n",
    "\n",
    "        # load the shape and pooled timeseries\n",
    "        shape = np.loadtxt(f'{cort_shape_path}/{id}.csv', delimiter=',').astype(int)\n",
    "        pooled = np.loadtxt(f'{cort_pooled_path}/{id}.csv', delimiter=',').reshape(shape)\n",
    "\n",
    "        # concatenate the timeseries\n",
    "        timeseries.append(pooled)\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "# concatenate into one time series\n",
    "def concat(timeseries):\n",
    "    concat_timeseries = timeseries[0]\n",
    "    for series in timeseries[1:]:\n",
    "        concat_timeseries = np.concatenate((concat_timeseries, series), axis=0)\n",
    "    return concat_timeseries\n",
    "\n",
    "# get separate ids for each group\n",
    "abide_groups = get_groups(abide_ids, phenotype)\n",
    "\n",
    "# get timeseries for each group\n",
    "asd_timeseries = get_timeseries(abide_groups['asd']) # asd timeseries\n",
    "tdc_timeseries = get_timeseries(abide_groups['tdc']) # tdc timeseries\n",
    "\n",
    "# all timeseries concatenated\n",
    "asd_concat = concat(asd_timeseries) # asd concatenated timeseries\n",
    "tdc_concat = concat(tdc_timeseries) # tdc concatenated timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get covariance matrix for a given timeseries\n",
    "def get_covariance(concat_timeseries):\n",
    "    \"\"\"\n",
    "    Get the covariance matrix of the concatenated timeseries.\n",
    "    \"\"\"\n",
    "    # get the covariance matrix\n",
    "    cov = np.cov(concat_timeseries.T)\n",
    "\n",
    "    for i in range(len(cov)):\n",
    "        for j in range(len(cov)):\n",
    "            if i == j:\n",
    "                cov[i][j] = 0\n",
    "\n",
    "    return cov\n",
    "\n",
    "# helper function for mutual info between two continuous random variables\n",
    "def mutual_information_continuous(x, y, bins=100):\n",
    "    \"\"\"\n",
    "    Estimate mutual information between two continuous variables by discretizing them.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): First continuous variable.\n",
    "    y (array-like): Second continuous variable.\n",
    "    bins (int): Number of bins to use for discretization.\n",
    "\n",
    "    Returns:\n",
    "    float: Estimated mutual information.\n",
    "    \"\"\"\n",
    "    # Convert to pandas Series\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Discretize the continuous values\n",
    "    x_binned = pd.cut(x, bins=bins, labels=False)\n",
    "    y_binned = pd.cut(y, bins=bins, labels=False)\n",
    "\n",
    "    # Compute contingency table\n",
    "    contingency_table = pd.crosstab(x_binned, y_binned)\n",
    "\n",
    "    # Compute mutual information\n",
    "    mi = mutual_info_score(None, None, contingency=contingency_table.values)\n",
    "\n",
    "    return mi\n",
    "\n",
    "# get mutual information between all regions for a given timeseries\n",
    "def get_mutual_info(timeseries, bins=1000):\n",
    "\n",
    "    mutual_info_matrix = np.zeros((timeseries.shape[1], timeseries.shape[1]))\n",
    "\n",
    "    # get mutual information\n",
    "    for region in range(timeseries.shape[1]):\n",
    "        for region2 in range(timeseries.shape[1]):\n",
    "            if region != region2:\n",
    "                mutual_info = mutual_information_continuous(timeseries[:, region], timeseries[:, region2], bins=bins)\n",
    "                mutual_info_matrix[region, region2] = mutual_info\n",
    "    \n",
    "    return mutual_info_matrix\n",
    "\n",
    "# Construct the Chow-Liu tree from continuous data using mutual information.\n",
    "def get_chow_liu_tree(timeseries, mi_matrix=None, bins=1000):\n",
    "    \n",
    "    # Compute the mutual information matrix\n",
    "    if mi_matrix is None:\n",
    "        mi_matrix = get_mutual_info(timeseries, bins=bins)\n",
    "\n",
    "    # Set the diagonal to zero (self-information)\n",
    "    np.fill_diagonal(mi_matrix, 0)\n",
    "\n",
    "    # We use the negative MI because scipy's minimum_spanning_tree computes *minimum* tree.\n",
    "    mst = minimum_spanning_tree(-mi_matrix).toarray()\n",
    "\n",
    "    # Make MST undirected and remove negative sign\n",
    "    mst = -mst + (-mst).T\n",
    "\n",
    "    # Create NetworkX graph for visualization or further use\n",
    "    G = nx.Graph()\n",
    "    n = mi_matrix.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if mst[i, j] != 0:\n",
    "                G.add_edge(i, j, weight=mi_matrix[i, j])\n",
    "    \n",
    "    return G, mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate timeseries for prediction dataset\n",
    "def truncate_timeseries(tdc_timeseries, asd_timeseries, fraction=0.1):\n",
    "\n",
    "    n_tdc = int(len(tdc_timeseries) * fraction)\n",
    "    n_asd = int(len(asd_timeseries) * fraction)\n",
    "\n",
    "    # select n_tdc random timeseries\n",
    "    tdc_indices = np.random.choice(len(tdc_timeseries), n_tdc, replace=False)\n",
    "    tdc_timeseries_trunc = [tdc_timeseries[i] for i in tdc_indices]\n",
    "\n",
    "    # select n_asd random timeseries\n",
    "    asd_indices = np.random.choice(len(asd_timeseries), n_asd, replace=False)\n",
    "    asd_timeseries_trunc = [asd_timeseries[i] for i in asd_indices]\n",
    "\n",
    "    return tdc_timeseries_trunc, asd_timeseries_trunc\n",
    "\n",
    "# get cross validation split\n",
    "def cross_validation_split(tdc_timeseries_trunc, asd_timeseries_trunc, hold_out = 0.2):\n",
    "\n",
    "    # initialize training and test sets\n",
    "    tdc_training_set = []\n",
    "    tdc_test_set = []\n",
    "    asd_training_set = []\n",
    "    asd_test_set = []\n",
    "\n",
    "    # get number of training and test samples\n",
    "    tdc_train_num = int(len(tdc_timeseries_trunc) * (1 - hold_out))\n",
    "    asd_train_num = int(len(asd_timeseries_trunc) * (1 - hold_out))\n",
    "\n",
    "    # randomly shuffle timeseries\n",
    "    tdc_indices = np.random.choice(len(tdc_timeseries_trunc), len(tdc_timeseries_trunc), replace=False)\n",
    "    asd_indices = np.random.choice(len(asd_timeseries_trunc), len(asd_timeseries_trunc), replace=False)\n",
    "\n",
    "    # split into training and test sets\n",
    "    tdc_train_indices = tdc_indices[:tdc_train_num]\n",
    "    tdc_test_indices = tdc_indices[tdc_train_num:]\n",
    "    asd_train_indices = asd_indices[:asd_train_num]\n",
    "    asd_test_indices = asd_indices[asd_train_num:]\n",
    "\n",
    "    # get training and test sets\n",
    "    for i in tdc_train_indices:\n",
    "        tdc_training_set.append(tdc_timeseries_trunc[i])\n",
    "    for i in tdc_test_indices:\n",
    "        tdc_test_set.append(tdc_timeseries_trunc[i])\n",
    "    for i in asd_train_indices:\n",
    "        asd_training_set.append(asd_timeseries_trunc[i])\n",
    "    for i in asd_test_indices:\n",
    "        asd_test_set.append(asd_timeseries_trunc[i])\n",
    "    \n",
    "    # return concatenated timeseries for each\n",
    "    tdc_train_concat = concat(tdc_training_set)\n",
    "    tdc_test_concat = concat(tdc_test_set)\n",
    "    asd_train_concat = concat(asd_training_set)\n",
    "    asd_test_concat = concat(asd_test_set)\n",
    "\n",
    "    data = {\"tdc_train\": tdc_train_concat, \"tdc_test\": tdc_test_concat, \"asd_train\": asd_train_concat, \"asd_test\": asd_test_concat}\n",
    "    return data\n",
    "\n",
    "# separate concat timeseries into bins to create train and test set\n",
    "def bin_data(data, bin_num = 100, hold_out = 0.2):\n",
    "\n",
    "    train_bin_num = int(bin_num * (1 - hold_out))\n",
    "    test_bin_num = int(bin_num * hold_out)\n",
    "\n",
    "    # bin data\n",
    "    tdc_train_bins = np.array_split(data['tdc_train'], train_bin_num)\n",
    "    tdc_test_bins = np.array_split(data['tdc_test'], test_bin_num)\n",
    "    asd_train_bins = np.array_split(data['asd_train'], train_bin_num)\n",
    "    asd_test_bins = np.array_split(data['asd_test'], test_bin_num)\n",
    "\n",
    "    # return data\n",
    "    data = {\"tdc_train\": tdc_train_bins, \"tdc_test\": tdc_test_bins, \"asd_train\": asd_train_bins, \"asd_test\": asd_test_bins}\n",
    "    return data\n",
    "\n",
    "# consolidate\n",
    "def data_split(tdc_timeseries, asd_timeseries, fraction=0.1, hold_out=0.2, bin_num=100):\n",
    "\n",
    "    # truncate timeseries\n",
    "    tdc_timeseries_trunc, asd_timeseries_trunc = truncate_timeseries(tdc_timeseries, asd_timeseries, fraction=fraction)\n",
    "\n",
    "    # get cross validation split\n",
    "    data = cross_validation_split(tdc_timeseries_trunc, asd_timeseries_trunc, hold_out=hold_out)\n",
    "\n",
    "    # bin data\n",
    "    data = bin_data(data, bin_num=bin_num, hold_out=hold_out)\n",
    "\n",
    "    return data\n",
    "\n",
    "# shuffle asd and tdc, assign labels\n",
    "def shuffle_labels(data):\n",
    "\n",
    "    # shuffle the labels\n",
    "    tdc_train_labels = np.zeros(len(data['tdc_train']))\n",
    "    tdc_test_labels = np.zeros(len(data['tdc_test']))\n",
    "    asd_train_labels = np.ones(len(data['asd_train']))\n",
    "    asd_test_labels = np.ones(len(data['asd_test']))\n",
    "\n",
    "    # concatenate the data and labels\n",
    "    train_data = data['tdc_train'] + data['asd_train']\n",
    "    test_data = data['tdc_test'] + data['asd_test']\n",
    "    train_labels = np.concatenate((tdc_train_labels, asd_train_labels), axis=0)\n",
    "    test_labels = np.concatenate((tdc_test_labels, asd_test_labels), axis=0)\n",
    "\n",
    "    # shuffle the data and labels\n",
    "    indices = np.arange(len(train_data))\n",
    "    np.random.shuffle(indices)\n",
    "    train_data = [train_data[i] for i in indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    indices = np.arange(len(test_data))\n",
    "    np.random.shuffle(indices)\n",
    "    test_data = [test_data[i] for i in indices]\n",
    "    test_labels = test_labels[indices]\n",
    "    \n",
    "    data = {\"train_data\": train_data, \"test_data\": test_data, \"train_labels\": train_labels, \"test_labels\": test_labels}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_prediction_model(data, model_name=\"gnb\", model=GaussianNB(), metric=\"covariance\", fraction=0.1, mi_bin_num=100):\n",
    "\n",
    "    # get the data\n",
    "    train_data = data['train_data']\n",
    "    test_data = data['test_data']\n",
    "    train_labels = data['train_labels']\n",
    "    test_labels = data['test_labels']\n",
    "\n",
    "    # calculate metric (feature extraction)\n",
    "    if metric == \"covariance\":\n",
    "        if os.path.exists(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/covariance/{fraction}/data.csv'):\n",
    "            print(\"Using precomputed covariance matrices.\")\n",
    "            # load precomputed covariance matrices\n",
    "            train_data = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/covariance/{fraction}/data.csv', delimiter=',')\n",
    "            test_data = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/covariance/{fraction}/test_data.csv', delimiter=',')\n",
    "            train_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/covariance/{fraction}/train_labels.csv', delimiter=',')\n",
    "            test_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/covariance/{fraction}/test_labels.csv', delimiter=',')\n",
    "        # if not, calculate covariance matrices\n",
    "        else:\n",
    "            print(\"Calculating covariance matrices.\")\n",
    "            train_data = np.array([get_covariance(series).flatten() for series in train_data])\n",
    "            test_data = np.array([get_covariance(series).flatten() for series in test_data])\n",
    "    \n",
    "    elif metric == \"mutual_info\":\n",
    "        if os.path.exists(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/data.csv'):\n",
    "            print(\"Using precomputed mutual information.\")\n",
    "            # load precomputed mutual information\n",
    "            train_data = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/data.csv', delimiter=',')\n",
    "            test_data = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/test_data.csv', delimiter=',')\n",
    "            train_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/train_labels.csv', delimiter=',')\n",
    "            test_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/test_labels.csv', delimiter=',')\n",
    "        \n",
    "        # if not, calculate mutual information\n",
    "        else:\n",
    "            \"calculating mutual info\"\n",
    "            train_data = np.array([get_mutual_info(series, bins=mi_bin_num).flatten() for series in train_data])\n",
    "            test_data = np.array([get_mutual_info(series, bins=mi_bin_num).flatten() for series in test_data])\n",
    "    \n",
    "    elif metric == \"chow_liu\":\n",
    "        if os.path.exists(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/data.csv'):\n",
    "            print(\"Using precomputed chow-liu trees.\")\n",
    "            # load precomputed chow-liu trees\n",
    "            train_data = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/data.csv', delimiter=',')\n",
    "            test_data = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/test_data.csv', delimiter=',')\n",
    "            train_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/train_labels.csv', delimiter=',')\n",
    "            test_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/test_labels.csv', delimiter=',')\n",
    "        \n",
    "        elif os.path.exists(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/data.csv'):\n",
    "            print(\"Using precomputed mutual information for chow-liu trees.\")\n",
    "            # load precomputed mutual information\n",
    "            train_mi = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/data.csv', delimiter=',')\n",
    "            test_mi = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/test_data.csv', delimiter=',')\n",
    "            train_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/train_labels.csv', delimiter=',')\n",
    "            test_labels = np.loadtxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/mutual_info/{fraction}/test_labels.csv', delimiter=',')\n",
    "\n",
    "            # reshape for msdl\n",
    "            train_mi = train_mi.reshape(train_mi.shape[0], 39, 39)\n",
    "            test_mi = test_mi.reshape(test_mi.shape[0], 39, 39)\n",
    "            \n",
    "            # get chow-liu trees\n",
    "            train_data = np.array([get_chow_liu_tree(train_data[series_num], mi_matrix=train_mi[series_num], bins=mi_bin_num)[1].flatten() for series_num in range(len(train_data))])\n",
    "            test_data = np.array([get_chow_liu_tree(test_data[series_num], mi_matrix=test_mi[series_num], bins=mi_bin_num)[1].flatten() for series_num in range(len(test_data))])\n",
    "        \n",
    "        else:\n",
    "            \"calculating mutual info chow-liu trees\"\n",
    "            train_data = np.array([get_chow_liu_tree(series, bins=mi_bin_num)[1].flatten() for series in train_data])\n",
    "            test_data = np.array([get_chow_liu_tree(series, bins=mi_bin_num)[1].flatten() for series in test_data])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Choose 'covariance', 'mutual_info', or 'chow-liu'.\")\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    # make predictions\n",
    "    train_predictions = model.predict(train_data)\n",
    "    test_predictions = model.predict(test_data)\n",
    "\n",
    "    # get accuracy\n",
    "    data = {\"train_data\": train_data, \"test_data\": test_data, \"train_labels\": train_labels, \"test_labels\": test_labels}\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "    return data, train_accuracy, test_accuracy\n",
    "\n",
    "def save_data(data, train_accuracy, test_accuracy, model_name = \"gnb\", metric=\"covariance\", fraction=0.1, hold_out=0.2, bin_num=100, mi_bin_num=10):\n",
    "    \"\"\"\n",
    "    Save the data and accuracies to CSV files.\n",
    "    \"\"\"\n",
    "    # create directory if it doesn't exist\n",
    "    if not os.path.exists(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}'):\n",
    "        os.makedirs(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}')\n",
    "\n",
    "    # save data\n",
    "    np.savetxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/data.csv', data['train_data'], delimiter=',')\n",
    "    np.savetxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/test_data.csv', data['test_data'], delimiter=',')\n",
    "    \n",
    "    # save labels\n",
    "    np.savetxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/train_labels.csv', data['train_labels'], delimiter=',')\n",
    "    np.savetxt(f'{base_dir}/code/functional_connectivity/abide/output/data/{model_name}/{metric}/{fraction}/test_labels.csv', data['test_labels'], delimiter=',')\n",
    "    \n",
    "    # save accuracies\n",
    "    with open(f'{base_dir}/code/functional_connectivity/abide/output/data//{model_name}/{metric}/{fraction}/accuracies.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Metric', 'Model Name', 'Fraction', 'Hold Out', 'Bin Num', 'MI Bin Num', 'Train Accuracy', 'Test Accuracy'])\n",
    "        writer.writerow([metric, model_name, fraction, hold_out, bin_num, mi_bin_num, train_accuracy, test_accuracy])\n",
    "    print(f\"Data and accuracies saved for metric: {metric}, fraction: {fraction}, hold_out: {hold_out}, bin_num: {bin_num}, mi_bin_num: {mi_bin_num}\")\n",
    "\n",
    "def run_model(tdc_timeseries, asd_timeseries, model_name=\"gnb\", metric=\"covariance\", fraction=0.1, hold_out=0.2, bin_num=1000, mi_bin_num=10):\n",
    "\n",
    "    if model_name == \"gnb\":\n",
    "        model = GaussianNB()\n",
    "    elif model_name == \"logistic\":\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Clearing the model\n",
    "    model.theta_ = None\n",
    "    model.sigma_ = None\n",
    "\n",
    "    # get the data\n",
    "    preproc_data = data_split(tdc_timeseries, asd_timeseries, fraction=fraction, hold_out=hold_out, bin_num=bin_num)\n",
    "\n",
    "    # shuffle the labels\n",
    "    labeled_data = shuffle_labels(preproc_data)\n",
    "\n",
    "    # calculate metric and return processed data\n",
    "    data, train_accuracy, test_accuracy = binary_prediction_model(labeled_data, model_name=model_name, model=model, metric=metric, fraction=fraction, mi_bin_num=mi_bin_num)\n",
    "\n",
    "    save_data(data, train_accuracy, test_accuracy, model_name=model_name, metric=metric, fraction=fraction, hold_out=hold_out, bin_num=bin_num, mi_bin_num=mi_bin_num)\n",
    "\n",
    "    return data, train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(fractions=[0.1], total_bin=1000):\n",
    "\n",
    "    for fraction in fractions:\n",
    "        # calculate bin_num based on fraction\n",
    "        bin_num = fraction*total_bin\n",
    "        print(f\"Running experiment with fraction: {fraction}, bin_num: {bin_num}\")\n",
    "        # run the model with covariance\n",
    "        run_model(tdc_timeseries, asd_timeseries, bin_num=bin_num, metric=\"covariance\", fraction=fraction)\n",
    "        # run the model with mutual information\n",
    "        run_model(tdc_timeseries, asd_timeseries, metric=\"mutual_info\", fraction=fraction, hold_out=0.2, bin_num=bin_num, mi_bin_num=10)\n",
    "        # run the model with chow-liu\n",
    "        run_model(tdc_timeseries, asd_timeseries, metric=\"chow_liu\", fraction=fraction, hold_out=0.2, bin_num=bin_num, mi_bin_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with fraction: 0.01, bin_num: 10.0\n",
      "Calculating covariance matrices.\n",
      "Data and accuracies saved for metric: covariance, fraction: 0.01, hold_out: 0.2, bin_num: 10.0, mi_bin_num: 10\n",
      "Data and accuracies saved for metric: mutual_info, fraction: 0.01, hold_out: 0.2, bin_num: 10.0, mi_bin_num: 10\n",
      "Using precomputed mutual information for chow-liu trees.\n",
      "Data and accuracies saved for metric: chow_liu, fraction: 0.01, hold_out: 0.2, bin_num: 10.0, mi_bin_num: 10\n",
      "Running experiment with fraction: 0.02, bin_num: 20.0\n",
      "Calculating covariance matrices.\n",
      "Data and accuracies saved for metric: covariance, fraction: 0.02, hold_out: 0.2, bin_num: 20.0, mi_bin_num: 10\n",
      "Data and accuracies saved for metric: mutual_info, fraction: 0.02, hold_out: 0.2, bin_num: 20.0, mi_bin_num: 10\n",
      "Using precomputed mutual information for chow-liu trees.\n",
      "Data and accuracies saved for metric: chow_liu, fraction: 0.02, hold_out: 0.2, bin_num: 20.0, mi_bin_num: 10\n",
      "Running experiment with fraction: 0.03, bin_num: 30.0\n",
      "Calculating covariance matrices.\n",
      "Data and accuracies saved for metric: covariance, fraction: 0.03, hold_out: 0.2, bin_num: 30.0, mi_bin_num: 10\n",
      "Data and accuracies saved for metric: mutual_info, fraction: 0.03, hold_out: 0.2, bin_num: 30.0, mi_bin_num: 10\n",
      "Using precomputed mutual information for chow-liu trees.\n",
      "Data and accuracies saved for metric: chow_liu, fraction: 0.03, hold_out: 0.2, bin_num: 30.0, mi_bin_num: 10\n",
      "Running experiment with fraction: 0.04, bin_num: 40.0\n",
      "Calculating covariance matrices.\n",
      "Data and accuracies saved for metric: covariance, fraction: 0.04, hold_out: 0.2, bin_num: 40.0, mi_bin_num: 10\n",
      "Data and accuracies saved for metric: mutual_info, fraction: 0.04, hold_out: 0.2, bin_num: 40.0, mi_bin_num: 10\n",
      "Using precomputed mutual information for chow-liu trees.\n",
      "Data and accuracies saved for metric: chow_liu, fraction: 0.04, hold_out: 0.2, bin_num: 40.0, mi_bin_num: 10\n"
     ]
    }
   ],
   "source": [
    "run_experiment(fractions=[0.01, 0.02, 0.03, 0.04])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
